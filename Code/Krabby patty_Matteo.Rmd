```{r}
library(tidyverse)
library(reshape2)
```

```{r}
ds1=read.csv("https://raw.githubusercontent.com/guber25/Crab_Age/main/Dataset/Crab_age.csv")
#https://www.kaggle.com/datasets/sidhus/crab-age-prediction
or_dim=dim(ds1)[1]
ds1[,1] <- as.factor(ds1[,1])
```

```{r}
Sex_M=ifelse(ds1$Sex=="M",yes = 1, 0)
Sex_F=ifelse(ds1$Sex=="F",yes = 1, 0)
Sex_I=ifelse(ds1$Sex=="I",yes = 1, 0)
ds1$Sex_M=Sex_M
ds1$Sex_F=Sex_F
ds1$Sex_I=Sex_I
```

```{r}
View(ds1)
```

```{r}
#We try to find some correlation between the variables. Clearly some correlation is present but there are many outliers.
plot(ds1)
```

### WHAT TO DO:

-   exploratory analysis: bella zi dai lo sapete, boxplots, histograms

-   linear regression -\> significativitÃ  delle variabili, collinearity

-   classification -\> regressione logistica e altri modelli che possono andare bene, fare alcuni test -\>confusion matrix

-   Unsupervised: k-means clustering ? :-D

```{r}
#In this way we look at the distribution of the variables and we see, again, the presence of outliers and of skewed distribution
ggplot(gather(select(ds1, -c("Sex"))), aes(value)) + 
    geom_histogram(bins = 10) + 
    facet_wrap(~key, scales = 'free') 
```

```{r}
glimpse(ds1)
```

```{r}
ggplot(gather(select(ds1, -c("Sex"))), aes(value)) + 
    geom_boxplot() + 
    facet_wrap(~key, scales = 'free')
```

```{r}
#we look if there are NULL values or NA values
cat("NULL values count:",sum(is.null(ds1)),"\nNA values count:", sum(is.na(ds1)))
```

#### Outlier removal

```{r}
# OUTLIER REMOVAL (substituting outliers with NA)
for (i in 1:3) {
for (x in ds1 %>% select(-"Sex") %>% names())
{
  value = ds1[,x][ds1[,x] %in% boxplot.stats(ds1[,x])$out]
  ds1[,x][ds1[,x] %in% value] = NA
  ds1 = drop_na(ds1)
} }
head(ds1)
```

```{r}
#Check if there are no more NA values
as.data.frame(colSums(is.na(ds1)))
```

```{r}
#as the plots show, there are no more outliers
ggplot(gather(select(ds1, -c("Sex"))), aes(value)) + 
    geom_boxplot() + 
    facet_wrap(~key, scales = 'free')
```

```{r}
cat(((or_dim-dim(ds1)[1])/or_dim)*100,"%")
```

Due to the removal of outliers, we have lost 9.86% of the data.

```{r}
# We see how outlier removal impacted distributions
ggplot(gather(select(ds1, -c("Sex"))), aes(value)) + 
    geom_histogram(bins = 10) + 
    facet_wrap(~key, scales = 'free')
```

```{r}
#Now correlations are much more stable
plot(ds1)
```

```{r}
#Correlation matrix
ds1 %>% select(-Sex) %>% cor() %>% melt() %>% #here I am creating the correlation matrix between each other variable (excluding Sex) and "melting" it to creating a data frame which will be used to then create the ggplot 
  ggplot(aes(x=Var2, y=Var1, fill=value)) + 
    geom_tile() +
    geom_text(aes(Var1, Var2, label = round(value, 2)), size = 3, color="white") +
      scale_fill_gradient2(low = "12123220", high = "darkblue",
                         limit = c(-1,1), name="Correlation") +
    theme(axis.text.x = element_text(angle = 45, vjust = .5), 
          axis.title.x = element_blank(),
          axis.title.y = element_blank(),
          panel.background = element_blank())
```

head(ds1)
ds2 = ds1 %>% select(-c("Sex", "Sex_M", "Sex_F", "Sex_I", "Age"))
head(ds2)

#how to use prcomp() function to do PCA 
#how to draw a PCA plot using "base graphics" and "ggplot()"
#how to determine how much variation each PC accounts for 
#how to examine the loading scores to determine what variables have the largest effect on the graph



##WHAT ARE THE PRINCIPAL COMPONENTS?
#The PCs are new variables that are constructed as linear combination or mixture of the initial variable.
#These combinations are done in such a way that the new variables are uncorrelated 
#and most of the information within the initial variable is compressed into the first components
#PCA tries to put maximum possible information in the PC1, then maximum remaining information in the PC2 and so on 
#until we have something like shown in the Scree Plot where we have the percentage of Variance (information) for each PC
#In this way we can reduce dimensionality without losing much information 
#Geometrically speaking the PCs represent the directions of the data that explain a maximal amount of variance (the lines that capture most information of the data)
#The relationship between Variance and Information here is that the larger the variance carried by a line, the larger the dispersion of the data points along it and the larger the dispersion along the line, the more information it has 
#Basically the PCs are the new axes that provide the best angel to see and evaluate the data so that the differences among the observations are better visible 




#Now we call "prcomp()" function to do PCA on our data 
#the goal is to draw a graph that shows how the samples are related (or not related) to each other
pca <- prcomp(ds2, scale = TRUE)
pca
names(pca)
summary(pca)


#by default the "prcomp()" function expects the samples to be rows and the variables to be columns 
#eventually we can use "t()" function to transpose the matrix

# "prcomp()" returns 3 things:
# 1) x: contains the PCs for drawing a graph. We can use the first 2 columns in x to draw a 2-D plot that uses the first 2 PCs. The first PC accounts for the most variation in the original data 
pca$x
plot(pca$x[,1], pca$x[,2])

#To get a sense let's see how much variation in the original data PC1 accounts for 


# 2) sdev
#So we use the square of the sdev (the standard deviation)
pca.var <- pca$sdev^2

#Let's calculate the percentage of variation that each PC accounts for (it's more interesting than the actual value)
pca.var.per <- round(pca.var/sum(pca.var)*100, 2)

#now the can plot the with "barplot()" function 
barplot(pca.var.per, main = "Scree Plot", xlab = "PCs", ylab = "% Variation")

#we can notice that PC1 account for almost all of the variation in the data

# 3) rotation
pca$rotation


#Let's plot the Scree Plot installing a new package "factoextra" and using the function "fviz_eig()"
#install.packages("factoextra")
library(factoextra)
fviz_eig(pca, 
         addlabels = TRUE,
         xlab = "Principal Components",
         ylim = c(0, 94),
         main = "Scree Plot")

#or with eigenvalues 
fviz_eig(pca,
         addlabels = TRUE,
         choice = "eigenvalue",
         main = "Scree Plot") + geom_hline(yintercept = 1,
                                           linetype = "dashed",
                                           color = "red")


#now we get off the PC result for the variables ("to extract the result for variables only)
var <- get_pca_var(pca)
var
help("get_pca_var")
ind <- get_pca_ind(pca)
#so to see the most important contributing variables for each dimension 
#and to do so we use the "corrplot()" 
library(corrplot)
corrplot(var$cos2, is.corr = FALSE)
#to see the most contributing variable for both dimension (for ex if you want for PC 1 and 2)
#and we need to use the function "fviz_cos2()" with "axes"
fviz_cos2(pca, choice = "var", axes = 1:2)

#To draw a barplot of variable contribution 
#contributions of variables to PC1 (red dashed line is the mean)
a <- fviz_contrib(pca, choice = "var", axes = 1)
a
#for PC2
b <- fviz_contrib(pca, choice = "var", axes = 2)
b

#if the number of variable is too high (like 90, 50,...) you can use "top" like top = 5 to display for 5 variables only 
fviz_contrib(pca, choice = "var", axes = 2, top = 3)

#Then we can arrange the PCs we selected 
library(gridExtra)
grid.arrange(a, b, ncol=2, top = "Contribution of the variables of the first two PCs")

#To see the total contribution of the PC 1 and 2 
#but in this case we don't use "var" but we use "ind" 
fviz_contrib(pca, choice = "ind", axes = 1)

fviz_pca_var(pca,
             col.var = "cos2",
             gradient.cols = c("red", "blue", "green"),
             repel = TRUE)




